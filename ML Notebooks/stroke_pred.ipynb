{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stroke Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749047988445,
     "user": {
      "displayName": "AJAY RAJAN A 2022-2026",
      "userId": "09789360077911701430"
     },
     "user_tz": -330
    },
    "id": "pCmOrJfNz5o8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df = pd.read_csv(\"/content/stroke_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1749047989192,
     "user": {
      "displayName": "AJAY RAJAN A 2022-2026",
      "userId": "09789360077911701430"
     },
     "user_tz": -330
    },
    "id": "VUwqHn9y0CRv",
    "outputId": "a8215dde-421f-43d1-f7cd-05bdbfb39467"
   },
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1749047990981,
     "user": {
      "displayName": "AJAY RAJAN A 2022-2026",
      "userId": "09789360077911701430"
     },
     "user_tz": -330
    },
    "id": "bOK42EY50FRR",
    "outputId": "79cd37e0-1ac7-4f06-9ce2-0101706d9f3d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>5110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypertension</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_disease</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ever_married</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work_type</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residence_type</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <td>3979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmi</th>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoking_status</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stroke</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "id                   5110\n",
       "gender                  3\n",
       "age                   104\n",
       "hypertension            2\n",
       "heart_disease           2\n",
       "ever_married            2\n",
       "work_type               5\n",
       "Residence_type          2\n",
       "avg_glucose_level    3979\n",
       "bmi                   418\n",
       "smoking_status          4\n",
       "stroke                  2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()\n",
    "print(\"Shape of the dataset:\", df.shape)\n",
    "print(\"\\nSummary statistics:\\n\", df.describe())\n",
    "print(\"\\nData types:\\n\", df.dtypes)\n",
    "print(\"\\nNumber of unique values per column:\\n\", df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nUnique values in 'work_type':\", df['work_type'].unique().tolist())\n",
    "print(\"Unique values in 'Residence_type':\", df['Residence_type'].unique().tolist())\n",
    "print(\"Unique values in 'smoking_status':\", df['smoking_status'].unique().tolist())\n",
    "print(\"Unique values in 'gender':\", df['gender'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nValue counts for 'gender':\\n\", df['gender'].value_counts())\n",
    "print(\"\\nValue counts for 'smoking_status':\\n\", df['smoking_status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty strings and similar with NaN\n",
    "df.replace([\"\", \" \", \"NA\", \"N/A\", \"None\", \"-\", \"nan\", \"null\", \"NULL\"], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values before handling:\\n\", df.isnull().sum())\n",
    "\n",
    "# Fill missing 'bmi' values with the mean\n",
    "df['bmi'] = df['bmi'].fillna(df['bmi'].mean())\n",
    "\n",
    "# Check for missing values after handling BMI\n",
    "print(\"\\nMissing values after handling BMI:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(\"\\nNumber of duplicate rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'id' column as it's not useful for modeling\n",
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers in 'avg_glucose_level' and 'bmi' using Z-score\n",
    "from scipy.stats import zscore\n",
    "z_score_glucose = zscore(df['avg_glucose_level'])\n",
    "z_score_bmi = zscore(df['bmi'])\n",
    "\n",
    "# Count outliers (abs(z-score) > 3)\n",
    "glucose_outliers_count = np.sum(np.abs(z_score_glucose) > 3)\n",
    "bmi_outliers_count = np.sum(np.abs(z_score_bmi) > 3)\n",
    "print(f\"\\nGlucose outliers (z > 3): {glucose_outliers_count}\")\n",
    "print(f\"BMI outliers (z > 3): {bmi_outliers_count}\")\n",
    "\n",
    "# Replace outliers with the mean\n",
    "df['avg_glucose_level'] = np.where(np.abs(z_score_glucose) > 3, df['avg_glucose_level'].mean(), df['avg_glucose_level'])\n",
    "df['bmi'] = np.where(np.abs(z_score_bmi) > 3, df['bmi'].mean(), df['bmi'])\n",
    "\n",
    "# Verify outliers are handled\n",
    "z_score_glucose = zscore(df['avg_glucose_level'])\n",
    "z_score_bmi = zscore(df['bmi'])\n",
    "glucose_outliers_count = np.sum(np.abs(z_score_glucose) > 3)\n",
    "bmi_outliers_count = np.sum(np.abs(z_score_bmi) > 3)\n",
    "print(f\"\\nRemaining Glucose outliers (z > 3): {glucose_outliers_count}\")\n",
    "print(f\"Remaining BMI outliers (z > 3): {bmi_outliers_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows after cleaning\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data (optional, but good practice)\n",
    "df.to_csv('cleaned_stroke_data.csv', index=False)\n",
    "print(\"Cleaned data saved to 'cleaned_stroke_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify important features based on correlation or domain knowledge\n",
    "# Based on your previous code and common practice, these features are likely important:\n",
    "selected_features = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n",
    "                     'work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status', 'stroke']\n",
    "\n",
    "# Create a new DataFrame with only the selected features\n",
    "df_selected = df[selected_features].copy()\n",
    "\n",
    "print(\"\\nDataFrame with selected features:\")\n",
    "print(df_selected.head())\n",
    "\n",
    "# Check data types of the selected features\n",
    "print(\"\\nData types of selected features:\\n\", df_selected.dtypes)\n",
    "\n",
    "# Ensure there are no NaNs in the selected feature DataFrame before splitting\n",
    "if df_selected.isnull().sum().sum() > 0:\n",
    "    print(\"\\nWarning: NaNs still found in selected feature data. Please check cleaning steps.\")\n",
    "    print(df_selected.isnull().sum())\n",
    "else:\n",
    "    print(\"\\nNo NaNs found in selected feature data. Proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y) using the selected features DataFrame\n",
    "X = df_selected.drop(columns=['stroke'])\n",
    "y = df_selected['stroke']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns within the selected features\n",
    "categorical = X.select_dtypes(include='object').columns.tolist()\n",
    "numerical = X.select_dtypes(include=np.number).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Categorical columns for preprocessing: {categorical}\")\n",
    "print(f\"Numerical columns for preprocessing: {numerical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Use ColumnTransformer to apply transformations\n",
    "# One-hot encode categorical features and scale numerical features\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical),\n",
    "    ('num', StandardScaler(), numerical)\n",
    "], remainder='passthrough') # Keep other columns (if any) as they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model pipeline: preprocessing followed by Logistic Regression\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model pipeline\n",
    "model_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, zero_division=1))\n",
    "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Define the filename for the saved model\n",
    "filename = 'stroke_pred.joblib'\n",
    "\n",
    "# Save the trained pipeline using joblib\n",
    "joblib.dump(model_pipeline, filename)\n",
    "\n",
    "print(f\"✅ Trained Logistic Regression model pipeline saved successfully to {filename}.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPJJj4cSQeuz564C16GTZxP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
